{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8p7LnnVHy4Vj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Create Dataset"
      ],
      "metadata": {
        "id": "RUbXi49Y2qW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic dataset (binary classification)\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=2, n_classes=2,\n",
        "    n_informative=2, n_redundant=0, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features (best practice for ANNs)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Q0FXzxAP2nRJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Custom Dataset Class"
      ],
      "metadata": {
        "id": "wS2ULHX3214Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)  # classification â†’ long\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create train/test datasets\n",
        "train_dataset = ClassificationDataset(X_train, y_train)\n",
        "test_dataset = ClassificationDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "xRTmDBwI21kL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. DataLoader with Important Parameters"
      ],
      "metadata": {
        "id": "o52j2iR_2_KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,          # mini-batch training\n",
        "    shuffle=True,           # shuffle for randomness\n",
        "    num_workers=2,          # parallel data loading (if CPU cores available)\n",
        "    pin_memory=True         # better performance when using GPU\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False           # no shuffle for evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "ss7Y2hRP2zFq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Define a Simple ANN"
      ],
      "metadata": {
        "id": "gfFmUZYq3GKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleANN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleANN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Instantiate model\n",
        "model = SimpleANN(input_dim=2, hidden_dim=16, output_dim=2)"
      ],
      "metadata": {
        "id": "59zyVJi33FgH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Loss & Optimizer"
      ],
      "metadata": {
        "id": "MwCyaqOU3Ps9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()       # classification loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "eC-dldT03SUm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Loop"
      ],
      "metadata": {
        "id": "yRmKKu-r3eTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, criterion, optimizer, epochs=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for X_batch, y_batch in loader:\n",
        "            # Forward\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            # Backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Metrics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(loader):.4f}, Acc: {acc:.2f}%\")\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqbNFvCT3dl3",
        "outputId": "53a258ff-c626-4700-a894-c29038f50b94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.4984, Acc: 75.38%\n",
            "Epoch 2/20, Loss: 0.3225, Acc: 86.12%\n",
            "Epoch 3/20, Loss: 0.3148, Acc: 86.62%\n",
            "Epoch 4/20, Loss: 0.3079, Acc: 87.25%\n",
            "Epoch 5/20, Loss: 0.2972, Acc: 87.88%\n",
            "Epoch 6/20, Loss: 0.2943, Acc: 88.00%\n",
            "Epoch 7/20, Loss: 0.2876, Acc: 87.88%\n",
            "Epoch 8/20, Loss: 0.2835, Acc: 88.88%\n",
            "Epoch 9/20, Loss: 0.2810, Acc: 88.38%\n",
            "Epoch 10/20, Loss: 0.2746, Acc: 88.50%\n",
            "Epoch 11/20, Loss: 0.2701, Acc: 89.25%\n",
            "Epoch 12/20, Loss: 0.2637, Acc: 89.50%\n",
            "Epoch 13/20, Loss: 0.2590, Acc: 89.88%\n",
            "Epoch 14/20, Loss: 0.2553, Acc: 90.00%\n",
            "Epoch 15/20, Loss: 0.2486, Acc: 90.00%\n",
            "Epoch 16/20, Loss: 0.2428, Acc: 90.50%\n",
            "Epoch 17/20, Loss: 0.2396, Acc: 90.62%\n",
            "Epoch 18/20, Loss: 0.2378, Acc: 90.88%\n",
            "Epoch 19/20, Loss: 0.2321, Acc: 90.75%\n",
            "Epoch 20/20, Loss: 0.2293, Acc: 91.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluation"
      ],
      "metadata": {
        "id": "a1ySRlHs3mca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_FfyxNi3sHf",
        "outputId": "5f0a25c9-9c65-4db4-c0c9-3420fa131ae3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 92.50%\n"
          ]
        }
      ]
    }
  ]
}