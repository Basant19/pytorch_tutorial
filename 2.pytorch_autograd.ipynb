{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "daesNNkPmfqo",
        "outputId": "cca0d6e6-6f32-449c-c28f-46456c63f2fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWhat is Autograd?\\n\\nAutograd is PyTorch’s automatic differentiation engine.\\nIt automatically computes gradients (derivatives) of tensors with respect to some scalar value (usually the loss in machine learning).\\n\\nThis is super useful for backpropagation in neural networks, where we need gradients to update weights.\\n\\n🔹 Key Concepts\\n\\n1) Tensor with requires_grad=True\\n\\nTells PyTorch: \"track all operations on this tensor, so I can compute gradients later.\"\\n\\nimport torch\\n\\nx = torch.tensor([2.0], requires_grad=True)\\n\\n\\n2)Computation Graph\\n\\nWhen you perform operations on tensors, PyTorch builds a graph of those operations.\\n\\nEach node in the graph is a tensor, and edges are functions (operations).\\n\\nExample:\\n\\ny = x**2 + 3*x + 1\\n\\n\\nHere, PyTorch builds a graph:\\n\\nx → (square) → (multiply by 3) → (add) → y\\n\\n\\n3)Backward Pass (.backward())\\n\\nIf y is a scalar, calling y.backward() computes dy/dx.\\n\\nThe gradient is stored in x.grad.\\n\\ny.backward()\\nprint(x.grad)  # derivative of y wrt x: dy/dx = 2x + 3 = 7\\n\\n\\n4) Gradient Accumulation\\n\\nBy default, PyTorch accumulates gradients in .grad.\\n\\nBefore each training step, we clear them with:\\n\\noptimizer.zero_grad()  # or x.grad.zero_()\\n\\n\\nwith torch.no_grad()\\n\\nUsed during inference to stop building the graph and save memory.\\n\\nwith torch.no_grad():\\n    y = x * 2\\n\\n\\n5) Detach Tensor (.detach())\\n\\nCreates a new tensor that shares data but is not tracked for gradients.\\n\\nz = x.detach()\\n\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "What is Autograd?\n",
        "\n",
        "Autograd is PyTorch’s automatic differentiation engine.\n",
        "It automatically computes gradients (derivatives) of tensors with respect to some scalar value (usually the loss in machine learning).\n",
        "\n",
        "This is super useful for backpropagation in neural networks, where we need gradients to update weights.\n",
        "PyTorch’s Autograd automates for you — instead of manually applying chain rule, it traces the graph and computes everything automatically.\n",
        "\n",
        "🔹 Key Concepts\n",
        "\n",
        "1) Tensor with requires_grad=True\n",
        "\n",
        "Tells PyTorch: \"track all operations on this tensor, so I can compute gradients later.\"\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "\n",
        "2)Computation Graph\n",
        "\n",
        "When you perform operations on tensors, PyTorch builds a graph of those operations.\n",
        "\n",
        "Each node in the graph is a tensor, and edges are functions (operations).\n",
        "\n",
        "Example:\n",
        "\n",
        "y = x**2 + 3*x + 1\n",
        "\n",
        "\n",
        "Here, PyTorch builds a graph:\n",
        "\n",
        "x → (square) → (multiply by 3) → (add) → y\n",
        "\n",
        "\n",
        "3)Backward Pass (.backward())\n",
        "\n",
        "If y is a scalar, calling y.backward() computes dy/dx.\n",
        "\n",
        "The gradient is stored in x.grad.\n",
        "\n",
        "y.backward()\n",
        "print(x.grad)  # derivative of y wrt x: dy/dx = 2x + 3 = 7\n",
        "\n",
        "\n",
        "4) Gradient Accumulation\n",
        "\n",
        "By default, PyTorch accumulates gradients in .grad.\n",
        "\n",
        "Before each training step, we clear them with:\n",
        "\n",
        "optimizer.zero_grad()  # or x.grad.zero_()\n",
        "\n",
        "\n",
        "with torch.no_grad()\n",
        "\n",
        "Used during inference to stop building the graph and save memory.\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = x * 2\n",
        "\n",
        "\n",
        "5) Detach Tensor (.detach())\n",
        "\n",
        "Creates a new tensor that shares data but is not tracked for gradients.\n",
        "\n",
        "z = x.detach()\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56-ynSPzpRcU"
      },
      "source": [
        "# Without auto grad in pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ikxI2D4oJ7o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Inputs\n",
        "x = torch.tensor(6.7)  # Input feature\n",
        "y = torch.tensor(0.0)  # True label (binary)\n",
        "\n",
        "w = torch.tensor(1.0)  # Weight\n",
        "b = torch.tensor(0.0)  # Bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Cross-Entropy Loss for scalar\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    epsilon = 1e-8  # To prevent log(0)\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ],
      "metadata": {
        "id": "_muDIj_KrcWv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "z = w * x + b  # Weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)  # Predicted probability\n",
        "\n",
        "# Compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)"
      ],
      "metadata": {
        "id": "JaYdAfBlre7R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGbL8zQprhOb",
        "outputId": "451e237b-eeb9-413d-ff5c-dcd012258122"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivatives:\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x  # dz/dw = x\n",
        "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ],
      "metadata": {
        "id": "l1VaMii4rjrG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5wXDKZkrmcQ",
        "outputId": "cef75ec4-9c7a-4042-c842-caa1345fb524"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with auto grad in pytorch"
      ],
      "metadata": {
        "id": "GgLxv2BUsFD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "XZ-6-SPZrmQq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "1NumFeOQrrhr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5LPKUeZruoQ",
        "outputId": "4a6badb3-4a10-4c24-a807-fe617c7091d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBia-usirw-m",
        "outputId": "8f34681a-2649-462b-81ff-509ca50edc7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = w*x + b\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CEqGKrCrzWN",
        "outputId": "79bfa346-c37b-439a-9d43-b1dfbc7ef030"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxJDwXM7rzE2",
        "outputId": "e8a5b956-1002-49de-efb8-3fa7ff2837c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ5tom4Ar8Ex",
        "outputId": "f71ac965-3580-4b9a-e80a-3b755c1eee70"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "pOUGeXazr-9d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zQAEXzMsAef",
        "outputId": "4b9afec9-7f50-421b-988c-daa59e9a651d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Problem Statement\n",
        "\n",
        "We want to train a simple linear model:\n",
        "\n",
        "𝑦=2𝑥+1\n",
        "\n",
        "y=2x+1\n",
        "\n",
        "Our goal: Given some training data, learn the parameters (weight w and bias b) using PyTorch autograd.\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "xF3vxTA_sMuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Training data (x, y) pairs\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "y = torch.tensor([3.0, 5.0, 7.0, 9.0])  # y = 2x + 1"
      ],
      "metadata": {
        "id": "qH22MWg-uGlD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random initialization of parameters\n",
        "w = torch.tensor(0.0, requires_grad=True)  # weight\n",
        "b = torch.tensor(0.0, requires_grad=True)  # bias"
      ],
      "metadata": {
        "id": "gsluqxvvuKkc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear model\n",
        "def model(x):\n",
        "    return w * x + b\n",
        "\n",
        "# Mean Squared Error (MSE)\n",
        "def mse(y_pred, y_true):\n",
        "    return ((y_pred - y_true)**2).mean()\n"
      ],
      "metadata": {
        "id": "nMOcLnZWuPII"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(10):\n",
        "    # Forward pass\n",
        "    y_pred = model(x)\n",
        "    loss = mse(y_pred, y)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()   # computes dloss/dw and dloss/db\n",
        "\n",
        "    # Update parameters (manual SGD)\n",
        "    with torch.no_grad():   # stop tracking updates\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Clear gradients (important!)\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: w={w.item():.4f}, b={b.item():.4f}, loss={loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqxgvEDwuSg3",
        "outputId": "2a3e1004-dc59-4ff0-aeeb-44e067359ff4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w=3.5000, b=1.2000, loss=41.0000\n",
            "Epoch 2: w=1.1500, b=0.4100, loss=18.4150\n",
            "Epoch 3: w=2.7200, b=0.9530, loss=8.2744\n",
            "Epoch 4: w=1.6635, b=0.6024, loss=3.7210\n",
            "Epoch 5: w=2.3671, b=0.8502, loss=1.6763\n",
            "Epoch 6: w=1.8914, b=0.6966, loss=0.7579\n",
            "Epoch 7: w=2.2060, b=0.8116, loss=0.3453\n",
            "Epoch 8: w=1.9912, b=0.7463, loss=0.1597\n",
            "Epoch 9: w=2.1313, b=0.8014, loss=0.0761\n",
            "Epoch 10: w=2.0337, b=0.7755, loss=0.0383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "     (x=2)        (w)        (b)\n",
        "        │           │          │\n",
        "        │           │          │\n",
        "        └─────┐   * │   ┌──────┘\n",
        "              │───────> mul\n",
        "              │         │\n",
        "              │         ▼\n",
        "              │      (wx)\n",
        "              │\n",
        "              │\n",
        "              │             add\n",
        "              └─────────────► (+) ──────► ŷ = wx + b\n",
        "                                      │\n",
        "                                      │\n",
        "                                      ▼\n",
        "                              (ŷ - y_true)\n",
        "                                      │\n",
        "                                      ▼\n",
        "                               square error\n",
        "                                      │\n",
        "                                      ▼\n",
        "                                    loss\n",
        "\n",
        "\n",
        "What happens during .backward()?\n",
        "\n",
        "PyTorch applies chain rule through this graph:\n",
        "\n",
        "Compute\n",
        "\n",
        "∂𝐿\n",
        "∂𝑦^=2(𝑦^−𝑦)∂y^∂L=2(y^−y)\n",
        "\n",
        "Flow gradient to w and b:\n",
        "\n",
        "∂𝐿∂𝑤=∂𝐿∂𝑦^⋅𝑥∂w∂L=∂y^∂L⋅x𝐿∂𝑏=∂𝐿∂𝑦^⋅1∂b∂=∂y^∂L⋅1\n",
        "\n",
        "PyTorch stores these in w.grad and b.grad.\n",
        "\n",
        "🔹 Gradient Flow Summary\n",
        "\n",
        "Forward pass: Build graph → compute outputs (ŷ) → compute loss.\n",
        "\n",
        "Backward pass: Start from loss → apply chain rule backwards → fill w.grad and b.grad.\n",
        "\n",
        "Update step: Adjust w and b using gradients.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "veOaR10-ufVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}